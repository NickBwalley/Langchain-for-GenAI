{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key from the environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Using LangChainâ€™s ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a blockchain-based platform that aims to connect language learners with native speakers for language exchange and tutoring. Users can find language partners, schedule video calls for language practice, and earn rewards for helping others learn their native language.\\n\\nReal-life example: Sarah is a native English speaker who wants to improve her Spanish skills. She signs up for LangChain and connects with Carlos, a native Spanish speaker who is learning English. They schedule regular video calls where they help each other practice their target languages. Sarah earns rewards for helping Carlos with English, which she can use to book tutoring sessions with native Spanish speakers.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 20, 'total_tokens': 142, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BHYpZXZMiCU0LqHx8PHZ675Erfo2y', 'finish_reason': 'stop', 'logprobs': None} id='run-0d59dc02-c1da-443b-be43-fe6bae6f47ac-0' usage_metadata={'input_tokens': 20, 'output_tokens': 122, 'total_tokens': 142, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "# write your prompt\n",
    "prompt = \"What is LangChain with simple explanations and a real-life example ?\"\n",
    "\n",
    "# print the response\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Using Other LLM Models Using HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Choose a model that fits your hardware\n",
    "model_id = \"google/flan-t5-small\"  # Lightweight but capable model\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.1,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Initialize LangChain wrapper\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Test the model\n",
    "print(\"Model loaded:\", model_id)\n",
    "print(llm.invoke(\"What is Deep Learning?\"))\n",
    "print(llm.invoke(\"Explain quantum computing in simple terms\"))\n",
    "print(llm.invoke(\"Write a short poem about artificial intelligence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Chaining Prompts With LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts template and chaining using langchain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0.9)\n",
    "\n",
    "# Prompt Template - let you generate prompts that accepts variable, \n",
    "# we can have multiple variables as well\n",
    "template = \"What is the impact on my health, if I eat {food} and drink {drink}?\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Here chains comes into picture to go beyond single llm call \n",
    "# and involve sequence of llm calls, and chains llms and prompt togetger\n",
    "# Now we initialize our chain with prompt and llm model reference\n",
    "chain = prompt | llm\n",
    "\n",
    "# here we are invok the chain with food parameter as Bread and drink parameter as wine.\n",
    "print(chain.invoke({\"food\" : \"Goat Meat\",\"drink\":\"Beer\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Chain Multiple Tasks in a Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "#first template and chain\n",
    "template = \"Which is the most {adjectective} building in the world ?\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#second template and chain with the first chain\n",
    "template_second = \"Tell me more about the {building}?\"\n",
    "prompt_second = PromptTemplate.from_template(template_second)\n",
    "chain_second = {\"noun\" : chain} | prompt_second | llm | StrOutputParser()\n",
    "\n",
    "#invoking the chains of calls passing the value to chain 1 parameter\n",
    "print(chain_second.invoke({\"adjectective\" : \"famous\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Adding Memory (Chatbot Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize model with memory\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Start chatting!\n",
    "print(conversation.invoke(\"Hello! How is weather today ?\")[\"response\"])\n",
    "print(conversation.invoke(\"Can I go out for biking today ?\")[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
